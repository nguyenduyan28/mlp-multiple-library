{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad02d4e8",
      "metadata": {
        "id": "ad02d4e8"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import numpy as np\n",
        "import os\n",
        "import gzip\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import tempfile\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37273e84-4c76-4788-99e4-a729a1e8bc40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37273e84-4c76-4788-99e4-a729a1e8bc40",
        "outputId": "1d75be37-00ab-4ec8-bcc5-8d9ba0dc66d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error not data files\n",
            "Error not data files\n",
            "Error not data files\n"
          ]
        }
      ],
      "source": [
        "def unpickle(file): # ref: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "  train_data = []\n",
        "  train_label = []\n",
        "  test_data = []\n",
        "  test_label = []\n",
        "  with tarfile.open(file, 'r:gz') as t:\n",
        "    t.extractall('./data')\n",
        "    is_test_file = False\n",
        "\n",
        "    for member in t.getnames():\n",
        "      if ('test_batch' in member):\n",
        "        is_test_file = True\n",
        "\n",
        "      filename = os.path.join('.','data', member)\n",
        "      try:\n",
        "        with open(filename, 'rb') as fo:\n",
        "          batch = pickle.load(fo, encoding='latin1')\n",
        "          data_file, labels_file = batch['data'], batch['labels']\n",
        "          if (is_test_file == False):\n",
        "            train_data.append(data_file)\n",
        "            train_label.append(labels_file)\n",
        "          else :\n",
        "            test_data.append(data_file)\n",
        "            test_label.append(labels_file)\n",
        "            is_test_file = False\n",
        "      except:\n",
        "        print(\"Error not data files\")\n",
        "  return np.concatenate(train_data), np.concatenate(test_data), np.concatenate(train_label), np.concatenate(test_label)\n",
        "\n",
        "\n",
        "train_data_raw, test_data_raw, train_label_raw, test_label_raw = unpickle('cifar-10-python.tar.gz')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d52b3ef-37ac-4032-a1d5-0898729dd2af",
      "metadata": {
        "id": "0d52b3ef-37ac-4032-a1d5-0898729dd2af"
      },
      "outputs": [],
      "source": [
        "# normalize [0, 255] --> [0, 1]\n",
        "\n",
        "def greyscale_img(img_data):\n",
        "  img_data_split = np.array(np.split(img_data, 3, axis=1))\n",
        "  normalize_arr = (np.mean(img_data_split, axis=0, keepdims=True) / 255).reshape(img_data_split.shape[1], img_data_split.shape[2])\n",
        "  return np.array(normalize_arr)\n",
        "\n",
        "\n",
        "def onehot_encoding(labels, num_of_classifier = 9):\n",
        "  labels_onehot = np.zeros((labels.shape[0], num_of_classifier + 1))\n",
        "  # for i in range(labels.shape[0]):\n",
        "  #   labels_onehot[i, labels[i]] = 1\n",
        "  # optimize version\n",
        "  labels_onehot[np.arange(labels.shape[0]), labels] = 1\n",
        "  return labels_onehot\n",
        "\n",
        "def add_ones(X):\n",
        "  ones = np.ones((X.shape[0], 1))\n",
        "  return np.concatenate((ones, X), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24e1427",
      "metadata": {
        "id": "a24e1427"
      },
      "outputs": [],
      "source": [
        "train_X_pre_val = (train_data_raw)\n",
        "test_X_pre_val = (test_data_raw)\n",
        "train_Y_pre_val = onehot_encoding(train_label_raw)\n",
        "test_Y_pre_val= onehot_encoding(test_label_raw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c797f21d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c797f21d",
        "outputId": "1d3942b7-ec4c-4a2c-f479-3175914f25d6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 3072 into shape (32,32)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6cd0f996a7e5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_cimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtemp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtemp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (32,32)"
          ]
        }
      ],
      "source": [
        "\n",
        "n_rimages = 10; n_cimages = 10\n",
        "padding = 2\n",
        "canvas = 0.5 * np.ones((n_rimages * (32 + 2 * padding), n_cimages * (32 + 2 * padding)))\n",
        "rand_idxs = np.random.permutation(np.arange(len(train_data_raw))[:n_rimages * n_cimages])\n",
        "for r in range(n_rimages):\n",
        "    for c in range(n_cimages):\n",
        "        i = r * n_cimages + c\n",
        "        image = train_data_raw[rand_idxs[i]].reshape(32, 32)\n",
        "        temp1 = r * (32 + 2 * padding) + padding\n",
        "        temp2 = c * (32 + 2 * padding) + padding\n",
        "        canvas[temp1:temp1 + 32, temp2:temp2 + 32] = image\n",
        "plt.imshow(canvas, cmap='gray', vmin=0, vmax=1)\n",
        "plt.grid(None); plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19689682",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19689682",
        "outputId": "717fca29-d418-48b3-c6a1-d1d43d458bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 3072)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "train_X_ones =(train_X_pre_val)\n",
        "print(train_X_ones.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yv73GiKWFYw7",
      "metadata": {
        "id": "Yv73GiKWFYw7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef3dbdc",
      "metadata": {
        "id": "5ef3dbdc"
      },
      "outputs": [],
      "source": [
        "train_X, val_X, train_Y, val_Y = train_test_split(train_X_ones, train_Y_pre_val, test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af2cb46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2af2cb46",
        "outputId": "3ed73235-3e8c-4438-e2bc-a0accf2ff87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(37500, 3072)\n",
            "(12500, 3072)\n",
            "(37500, 10)\n",
            "(12500, 10)\n"
          ]
        }
      ],
      "source": [
        "print(train_X.shape)\n",
        "print(val_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(val_Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb268bd",
      "metadata": {
        "id": "5bb268bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "933b0bae",
      "metadata": {
        "id": "933b0bae"
      },
      "source": [
        "## Section Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a74c8a",
      "metadata": {
        "id": "66a74c8a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e362cb2",
      "metadata": {
        "id": "4e362cb2"
      },
      "outputs": [],
      "source": [
        "class MLP_tensorflow:\n",
        "  def __init__(self, num_of_layers = 3, num_of_node_input = train_X.shape[1], num_of_node_output = 10):\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.num_of_node_input = num_of_node_input\n",
        "    self.num_of_node_output = num_of_node_output\n",
        "    self.num_of_node_hidden = (2 / 3) * (self.num_of_node_input + self.num_of_node_output)\n",
        "    self.lr = 0.02\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba849e68",
      "metadata": {
        "id": "ba849e68"
      },
      "outputs": [],
      "source": [
        "# model = tf.keras.models.Sequential()\n",
        "\n",
        "# model.add(tf.keras.Input(train_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8109c046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8109c046",
        "outputId": "db9d79c3-dcfc-4835-de14-2ca76330f266"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.2846 - loss: 2.0916 - val_accuracy: 0.3486 - val_loss: 1.8429\n",
            "Epoch 2/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3898 - loss: 1.7037 - val_accuracy: 0.2822 - val_loss: 2.0821\n",
            "Epoch 3/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4246 - loss: 1.6198 - val_accuracy: 0.3727 - val_loss: 1.7449\n",
            "Epoch 4/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4371 - loss: 1.5727 - val_accuracy: 0.3558 - val_loss: 1.8049\n",
            "Epoch 5/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4482 - loss: 1.5387 - val_accuracy: 0.4223 - val_loss: 1.6240\n",
            "Epoch 6/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4574 - loss: 1.5139 - val_accuracy: 0.4312 - val_loss: 1.6399\n",
            "Epoch 7/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.4657 - loss: 1.4916 - val_accuracy: 0.3594 - val_loss: 1.8544\n",
            "Epoch 8/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4631 - loss: 1.4941 - val_accuracy: 0.4163 - val_loss: 1.7272\n",
            "Epoch 9/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4711 - loss: 1.4817 - val_accuracy: 0.4460 - val_loss: 1.6451\n",
            "Epoch 10/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4629 - loss: 1.4928 - val_accuracy: 0.4621 - val_loss: 1.6258\n",
            "Epoch 11/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4743 - loss: 1.4712 - val_accuracy: 0.4203 - val_loss: 1.7217\n",
            "Epoch 12/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4781 - loss: 1.4594 - val_accuracy: 0.4523 - val_loss: 1.6404\n",
            "Epoch 13/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4873 - loss: 1.4467 - val_accuracy: 0.4492 - val_loss: 1.6799\n",
            "Epoch 14/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4868 - loss: 1.4357 - val_accuracy: 0.4592 - val_loss: 1.6187\n",
            "Epoch 15/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4946 - loss: 1.4206 - val_accuracy: 0.4402 - val_loss: 1.6664\n",
            "Epoch 16/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4941 - loss: 1.4282 - val_accuracy: 0.4741 - val_loss: 1.6080\n",
            "Epoch 17/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4948 - loss: 1.4213 - val_accuracy: 0.4881 - val_loss: 1.5591\n",
            "Epoch 18/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5002 - loss: 1.4192 - val_accuracy: 0.4850 - val_loss: 1.5652\n",
            "Epoch 19/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5037 - loss: 1.3908 - val_accuracy: 0.4884 - val_loss: 1.5789\n",
            "Epoch 20/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5060 - loss: 1.3939 - val_accuracy: 0.4710 - val_loss: 1.6036\n",
            "Epoch 21/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5093 - loss: 1.3849 - val_accuracy: 0.4775 - val_loss: 1.5792\n",
            "Epoch 22/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4974 - loss: 1.4045 - val_accuracy: 0.4643 - val_loss: 1.6315\n",
            "Epoch 23/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5064 - loss: 1.3841 - val_accuracy: 0.4514 - val_loss: 1.6990\n",
            "Epoch 24/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5042 - loss: 1.3823 - val_accuracy: 0.4439 - val_loss: 1.7142\n",
            "Epoch 25/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5085 - loss: 1.3756 - val_accuracy: 0.4584 - val_loss: 1.6390\n",
            "Epoch 26/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5174 - loss: 1.3533 - val_accuracy: 0.4813 - val_loss: 1.5781\n",
            "Epoch 27/200\n",
            "\u001b[1m293/293\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5135 - loss: 1.3669 - val_accuracy: 0.4632 - val_loss: 2.0111\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 17.\n",
            "Training time: 48.51 seconds\n",
            "Validation Loss: 1.5591\n",
            "Validation Accuracy: 48.81%\n",
            "Test Loss: 1.7916\n",
            "Test Accuracy: 48.50%\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[676  27  45  41  23  13  63  58 167 112]\n",
            " [ 77 637  31  38  16  12  53  29  82 263]\n",
            " [137  15 450  93  58  57 249  96  44  28]\n",
            " [ 60  16  97 396  25 115 315  71  57  74]\n",
            " [ 81   7 195  69 364  29 325 131  45  33]\n",
            " [ 49  16 126 284  45 349 251  88  54  40]\n",
            " [ 21  16  70  80  42  30 933  29  19  38]\n",
            " [ 50  15  75  72  59  59 109 690  30  92]\n",
            " [134  61  18  43  12   8  19  13 786 119]\n",
            " [ 65 126  20  33  12  23  56  48  58 820]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.50      0.55      0.53      1225\n",
            "     Class 1       0.68      0.51      0.59      1238\n",
            "     Class 2       0.40      0.37      0.38      1227\n",
            "     Class 3       0.34      0.32      0.33      1226\n",
            "     Class 4       0.55      0.28      0.38      1279\n",
            "     Class 5       0.50      0.27      0.35      1302\n",
            "     Class 6       0.39      0.73      0.51      1278\n",
            "     Class 7       0.55      0.55      0.55      1251\n",
            "     Class 8       0.59      0.65      0.62      1213\n",
            "     Class 9       0.51      0.65      0.57      1261\n",
            "\n",
            "    accuracy                           0.49     12500\n",
            "   macro avg       0.50      0.49      0.48     12500\n",
            "weighted avg       0.50      0.49      0.48     12500\n",
            "\n",
            "GPU memory used: 29.57 MB\n",
            "Model saved as 'mlp_cifar10_model.h5'\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Assuming train_X, val_X, train_Y, val_Y are preloaded and have the following shapes:\n",
        "# train_X: (37500, 1025), val_X: (12500, 1025)\n",
        "# train_Y: (37500, 10), val_Y: (12500, 10)\n",
        "\n",
        "\n",
        "# Define the MLP model\n",
        "def build_mlp(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Initialize model\n",
        "input_dim = train_X.shape[1]\n",
        "output_dim = train_Y.shape[1]\n",
        "model = build_mlp(input_dim, output_dim)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    train_X, train_Y,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(val_X, val_Y),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(val_X, val_Y, verbose=0)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_X_pre_val, test_Y_pre_val, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy* 100:.2f}%\")\n",
        "# Predictions and metrics\n",
        "y_pred = np.argmax(model.predict(val_X), axis=-1)\n",
        "y_true = np.argmax(val_Y, axis=-1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "class_report = classification_report(y_true, y_pred, target_names=[\n",
        "    f\"Class {i}\" for i in range(10)\n",
        "])\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    memory_info = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    print(f\"GPU memory used: {memory_info['current'] / 1024**2:.2f} MB\")\n",
        "\n",
        "# Save model\n",
        "model.save(\"mlp_cifar10_model.h5\")\n",
        "print(\"Model saved as 'mlp_cifar10_model.h5'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d3qH7jr8NG-a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3qH7jr8NG-a",
        "outputId": "4b4fb1a8-32e3-4957-c639-d88eefbafdf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Val Loss: 1.8005, Val Acc: 0.4025\n",
            "Epoch 2, Val Loss: 1.7066, Val Acc: 0.4469\n",
            "Epoch 3, Val Loss: 1.6428, Val Acc: 0.4717\n",
            "Epoch 4, Val Loss: 1.6042, Val Acc: 0.4833\n",
            "Epoch 5, Val Loss: 1.5859, Val Acc: 0.4901\n",
            "Epoch 6, Val Loss: 1.5673, Val Acc: 0.5016\n",
            "Epoch 7, Val Loss: 1.5518, Val Acc: 0.5074\n",
            "Epoch 8, Val Loss: 1.5517, Val Acc: 0.5112\n",
            "Epoch 9, Val Loss: 1.5442, Val Acc: 0.5106\n",
            "Epoch 10, Val Loss: 1.5406, Val Acc: 0.5146\n",
            "Epoch 11, Val Loss: 1.5365, Val Acc: 0.5190\n",
            "Epoch 12, Val Loss: 1.5348, Val Acc: 0.5181\n",
            "Epoch 13, Val Loss: 1.5340, Val Acc: 0.5187\n",
            "Epoch 14, Val Loss: 1.5338, Val Acc: 0.5195\n",
            "Epoch 15, Val Loss: 1.5320, Val Acc: 0.5211\n",
            "Epoch 16, Val Loss: 1.5326, Val Acc: 0.5211\n",
            "Epoch 17, Val Loss: 1.5325, Val Acc: 0.5210\n",
            "Epoch 18, Val Loss: 1.5320, Val Acc: 0.5206\n",
            "Epoch 19, Val Loss: 1.5322, Val Acc: 0.5203\n",
            "Epoch 20, Val Loss: 1.5321, Val Acc: 0.5206\n",
            "Epoch 21, Val Loss: 1.5322, Val Acc: 0.5214\n",
            "Epoch 22, Val Loss: 1.5322, Val Acc: 0.5213\n",
            "Epoch 23, Val Loss: 1.5321, Val Acc: 0.5214\n",
            "Epoch 24, Val Loss: 1.5322, Val Acc: 0.5217\n",
            "Epoch 25, Val Loss: 1.5322, Val Acc: 0.5219\n",
            "Early stopping triggered\n",
            "Training time: 27.20 seconds\n",
            "Test Accuracy: 53.16%\n",
            "Confusion Matrix:\n",
            " [[662  58  71  45  37  22  33  44 164  89]\n",
            " [ 49 805  20  21  19  16  23  33  75 177]\n",
            " [109  30 467 105 146 101 125  94  33  17]\n",
            " [ 35  35 100 437  54 224 168  83  42  48]\n",
            " [ 58  27 191  76 510  79 129 146  38  25]\n",
            " [ 20  21 135 302  46 507 104 110  32  25]\n",
            " [ 10  43  88 134 114  62 742  44  14  27]\n",
            " [ 39  17  51  72  88  80  30 801  20  53]\n",
            " [130  83  19  32  20  13   4   9 829  74]\n",
            " [ 44 209  17  41  13  25  33  67  58 754]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.57      0.54      0.56      1225\n",
            "     Class 1       0.61      0.65      0.63      1238\n",
            "     Class 2       0.40      0.38      0.39      1227\n",
            "     Class 3       0.35      0.36      0.35      1226\n",
            "     Class 4       0.49      0.40      0.44      1279\n",
            "     Class 5       0.45      0.39      0.42      1302\n",
            "     Class 6       0.53      0.58      0.56      1278\n",
            "     Class 7       0.56      0.64      0.60      1251\n",
            "     Class 8       0.64      0.68      0.66      1213\n",
            "     Class 9       0.58      0.60      0.59      1261\n",
            "\n",
            "    accuracy                           0.52     12500\n",
            "   macro avg       0.52      0.52      0.52     12500\n",
            "weighted avg       0.52      0.52      0.52     12500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "import optax\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Assuming train_X, val_X, train_Y, val_Y, test_X_pre_val, test_Y_pre_val are preloaded\n",
        "# Normalize the dataset\n",
        "train_X = (train_X - train_X.mean(axis=0)) / train_X.std(axis=0)\n",
        "val_X = (val_X - val_X.mean(axis=0)) / val_X.std(axis=0)\n",
        "test_X_pre_val = (test_X_pre_val - test_X_pre_val.mean(axis=0)) / test_X_pre_val.std(axis=0)\n",
        "\n",
        "# Define the model with Xavier initialization\n",
        "def init_params(key, input_dim, output_dim):\n",
        "    key1, key2, key3, key4 = random.split(key, 4)\n",
        "    params = {\n",
        "        'W1': random.uniform(key1, (input_dim, 512), minval=-jnp.sqrt(6 / (input_dim + 512)), maxval=jnp.sqrt(6 / (input_dim + 512))),\n",
        "        'b1': jnp.zeros((512,)),\n",
        "        'W2': random.uniform(key2, (512, 256), minval=-jnp.sqrt(6 / (512 + 256)), maxval=jnp.sqrt(6 / (512 + 256))),\n",
        "        'b2': jnp.zeros((256,)),\n",
        "        'W3': random.uniform(key3, (256, 128), minval=-jnp.sqrt(6 / (256 + 128)), maxval=jnp.sqrt(6 / (256 + 128))),\n",
        "        'b3': jnp.zeros((128,)),\n",
        "        'W4': random.uniform(key4, (128, output_dim), minval=-jnp.sqrt(6 / (128 + output_dim)), maxval=jnp.sqrt(6 / (128 + output_dim))),\n",
        "        'b4': jnp.zeros((output_dim,))\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def forward(params, x, is_training=True, dropout_rate=0.3):\n",
        "    rng = random.PRNGKey(42)  # Use a random key for reproducibility\n",
        "    x = jnp.dot(x, params['W1']) + params['b1']\n",
        "    x = jax.nn.relu(x)\n",
        "    if is_training:\n",
        "        mask = random.bernoulli(rng, p=1-dropout_rate, shape=x.shape)\n",
        "        x = x * mask / (1 - dropout_rate)\n",
        "    x = jnp.dot(x, params['W2']) + params['b2']\n",
        "    x = jax.nn.relu(x)\n",
        "    if is_training:\n",
        "        mask = random.bernoulli(rng, p=1-dropout_rate, shape=x.shape)\n",
        "        x = x * mask / (1 - dropout_rate)\n",
        "    x = jnp.dot(x, params['W3']) + params['b3']\n",
        "    x = jax.nn.relu(x)\n",
        "    x = jnp.dot(x, params['W4']) + params['b4']\n",
        "    return jax.nn.softmax(x)\n",
        "\n",
        "# Loss and accuracy functions\n",
        "def loss_fn(params, x, y):\n",
        "    preds = forward(params, x)\n",
        "    return -jnp.mean(jnp.sum(y * jnp.log(preds + 1e-7), axis=1))\n",
        "\n",
        "def accuracy_fn(params, x, y):\n",
        "    preds = forward(params, x, is_training=False)\n",
        "    return jnp.mean(jnp.argmax(preds, axis=1) == jnp.argmax(y, axis=1))\n",
        "\n",
        "# Initialize parameters\n",
        "key = random.PRNGKey(42)\n",
        "input_dim = train_X.shape[1]\n",
        "output_dim = train_Y.shape[1]\n",
        "params = init_params(key, input_dim, output_dim)\n",
        "\n",
        "# Optimizer with learning rate scheduling\n",
        "lr_schedule = optax.exponential_decay(init_value=0.001, transition_steps=100, decay_rate=0.9)\n",
        "optimizer = optax.chain(optax.adam(lr_schedule))\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "@jit\n",
        "def update(params, opt_state, x, y):\n",
        "    grads = grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "# Training loop\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "patience = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle data\n",
        "    permutation = np.random.permutation(train_X.shape[0])\n",
        "    train_X_shuffled = train_X[permutation]\n",
        "    train_Y_shuffled = train_Y[permutation]\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, train_X.shape[0], batch_size):\n",
        "        x_batch = train_X_shuffled[i:i+batch_size]\n",
        "        y_batch = train_Y_shuffled[i:i+batch_size]\n",
        "        params, opt_state = update(params, opt_state, x_batch, y_batch)\n",
        "\n",
        "    # Validation\n",
        "    val_loss = loss_fn(params, val_X, val_Y)\n",
        "    val_acc = accuracy_fn(params, val_X, val_Y)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_params = params\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "test_acc = accuracy_fn(best_params, test_X_pre_val, test_Y_pre_val)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Predictions and metrics\n",
        "val_preds = jnp.argmax(forward(best_params, val_X, is_training=False), axis=1)\n",
        "val_true = jnp.argmax(val_Y, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(val_true, val_preds)\n",
        "class_report = classification_report(val_true, val_preds, target_names=[\n",
        "    f\"Class {i}\" for i in range(10)\n",
        "])\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rhxYWPPSNQLl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "rhxYWPPSNQLl",
        "outputId": "d18824e7-36bc-42de-d1c1-8829d7973773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mxnet-mkl==1.6.0\n",
            "  Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting numpy==1.23.1\n",
            "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.32.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.12.14)\n",
            "Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, mxnet-mkl\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
            "bigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.23.1 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n",
            "mizani 0.13.1 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "plotnine 0.14.4 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.23.1 which is incompatible.\n",
            "scikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n",
            "tensorflow 2.17.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.1 which is incompatible.\n",
            "xarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mxnet-mkl-1.6.0 numpy-1.23.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f94f1d764eaa4da4ad89df321b823e7b",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install mxnet-mkl==1.6.0 numpy==1.23.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MGzEbM_VNYTv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGzEbM_VNYTv",
        "outputId": "4865f67d-4625-4d30-f0f2-b5148c600b7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chainer\n",
            "  Downloading chainer-7.8.1.tar.gz (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from chainer) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from chainer) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from chainer) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from chainer) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from chainer) (4.25.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from chainer) (1.17.0)\n",
            "Building wheels for collected packages: chainer\n",
            "  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chainer: filename=chainer-7.8.1-py3-none-any.whl size=971816 sha256=dc72ce5b754c5bee675bde195358d66d2dbdb18913deb11026c94d478ad57397\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/95/6a/16014db6f761c4e742755b64aac60dbe142da1df6c5919f790\n",
            "Successfully built chainer\n",
            "Installing collected packages: chainer\n",
            "Successfully installed chainer-7.8.1\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install chainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "niA3CsxHYxK5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "niA3CsxHYxK5",
        "outputId": "30285de4-a302-455e-c91e-73646174fa6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: PYTHONUTF8=1\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-6868e1ed856c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PYTHONUTF8=1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install chainer==7.8.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "%env PYTHONUTF8=1\n",
        "!pip install chainer==7.8.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_etomYOY9iG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_etomYOY9iG",
        "outputId": "b2ef9bff-53b4-4198-9547-e710b66aebb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.32.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.12.14)\n",
            "Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, graphviz, mxnet\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.1\n",
            "    Uninstalling numpy-2.2.1:\n",
            "      Successfully uninstalled numpy-2.2.1\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.3\n",
            "    Uninstalling graphviz-0.20.3:\n",
            "      Successfully uninstalled graphviz-0.20.3\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1 numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_NcqgoCDZEiB",
      "metadata": {
        "id": "_NcqgoCDZEiB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "grad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
